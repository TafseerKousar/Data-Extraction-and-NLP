{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1301fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import random\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13df3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize session with retry mechanism\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[500, 502, 503, 504, 404, 403],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        self.session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = 'output_files'\n",
    "        self.article_dir = os.path.join(self.output_dir, 'extracted_articles')\n",
    "        self.create_directories()\n",
    "        \n",
    "        # Load stop words and sentiment words\n",
    "        self.stop_words = self.load_stop_words()\n",
    "        self.positive_words = self.load_word_list('positive-words.txt')\n",
    "        self.negative_words = self.load_word_list('negative-words.txt')\n",
    "        \n",
    "        # Cache for failed URLs\n",
    "        self.failed_urls = {}\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.article_dir, exist_ok=True)\n",
    "\n",
    "    def read_file_with_encoding(self, file_path):\n",
    "        \"\"\"Try different encodings to read the file\"\"\"\n",
    "        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    return f.read().lower()\n",
    "            except (UnicodeDecodeError, UnicodeError):\n",
    "                continue\n",
    "        \n",
    "        print(f\"Warning: Could not read file {file_path} with any encoding\")\n",
    "        return \"\"\n",
    "\n",
    "    def load_stop_words(self):\n",
    "        stop_words = set()\n",
    "        stop_word_files = [\n",
    "            'StopWords_Auditor.txt',\n",
    "            'StopWords_Currencies.txt',\n",
    "            'StopWords_DatesandNumbers.txt',\n",
    "            'StopWords_Generic.txt',\n",
    "            'StopWords_GenericLong.txt',\n",
    "            'StopWords_Geographic.txt',\n",
    "            'StopWords_Names.txt'\n",
    "        ]\n",
    "        \n",
    "        for file in stop_word_files:\n",
    "            try:\n",
    "                content = self.read_file_with_encoding(file)\n",
    "                if content:\n",
    "                    words = content.split()\n",
    "                    stop_words.update(words)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {file} not found\")\n",
    "        \n",
    "        return stop_words | set(stopwords.words('english'))\n",
    "    \n",
    "    def load_word_list(self, filename):\n",
    "        try:\n",
    "            content = self.read_file_with_encoding(filename)\n",
    "            if content:\n",
    "                words = content.split()\n",
    "                return set(word for word in words if word not in self.stop_words)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {filename} not found\")\n",
    "            return set()\n",
    "\n",
    "    def extract_article(self, url):\n",
    "        try:\n",
    "            if url in self.failed_urls:\n",
    "                print(f\"Skipping previously failed URL: {url}\")\n",
    "                return self.failed_urls[url]\n",
    "\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'DNT': '1',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Upgrade-Insecure-Requests': '1',\n",
    "            }\n",
    "            \n",
    "            response = self.session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            response.encoding = response.apparent_encoding\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            for script in soup(['script', 'style', 'meta', 'link']):\n",
    "                script.decompose()\n",
    "            \n",
    "            title = \"\"\n",
    "            title_tags = soup.find_all(['h1', 'title'])\n",
    "            for tag in title_tags:\n",
    "                if tag.text.strip():\n",
    "                    title = tag.text.strip()\n",
    "                    break\n",
    "            \n",
    "            article_text = \"\"\n",
    "            article_containers = soup.find_all(['article', 'main', 'div'], \n",
    "                                            class_=['article', 'content', 'post', 'entry-content'])\n",
    "            \n",
    "            if article_containers:\n",
    "                for container in article_containers:\n",
    "                    paragraphs = container.find_all('p')\n",
    "                    article_text += ' '.join(p.text.strip() for p in paragraphs)\n",
    "            \n",
    "            if not article_text:\n",
    "                main_content = soup.find(['main', 'div'], \n",
    "                                       {'id': ['main-content', 'content', 'article']})\n",
    "                if main_content:\n",
    "                    paragraphs = main_content.find_all('p')\n",
    "                    article_text += ' '.join(p.text.strip() for p in paragraphs)\n",
    "            \n",
    "            if not article_text:\n",
    "                paragraphs = soup.find_all('p')\n",
    "                article_text = ' '.join(p.text.strip() for p in paragraphs)\n",
    "            \n",
    "            if not article_text.strip():\n",
    "                raise ValueError(\"No article content extracted\")\n",
    "            \n",
    "            return title, article_text\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error extracting article: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            self.failed_urls[url] = (\"\", \"\")\n",
    "            return \"\", \"\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        words = word_tokenize(text)\n",
    "        cleaned_words = [word for word in words if word not in self.stop_words]\n",
    "        return ' '.join(cleaned_words)\n",
    "\n",
    "    def count_syllables(self, word):\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        \n",
    "        if word.endswith('es') or word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "        \n",
    "        prev_char_is_vowel = False\n",
    "        for char in word:\n",
    "            is_vowel = char in vowels\n",
    "            if is_vowel and not prev_char_is_vowel:\n",
    "                count += 1\n",
    "            prev_char_is_vowel = is_vowel\n",
    "            \n",
    "        return max(1, count)\n",
    "\n",
    "    def analyze_text(self, text):\n",
    "        if not text.strip():\n",
    "            return None\n",
    "            \n",
    "        cleaned_text = self.clean_text(text)\n",
    "        words = word_tokenize(cleaned_text)\n",
    "        \n",
    "        if not words:\n",
    "            return None\n",
    "            \n",
    "        sentences = sent_tokenize(text)\n",
    "        if not sentences:\n",
    "            sentences = [text]\n",
    "        \n",
    "        positive_score = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_score = sum(1 for word in words if word in self.negative_words)\n",
    "        \n",
    "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        complex_words = [word for word in words if self.count_syllables(word) > 2]\n",
    "        percent_complex_words = len(complex_words) / len(words) if words else 0\n",
    "        fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "        \n",
    "        syllable_count = sum(self.count_syllables(word) for word in words)\n",
    "        syllable_per_word = syllable_count / len(words) if words else 0\n",
    "        \n",
    "        pronouns = re.findall(r'\\b(i|we|my|ours|us)\\b', cleaned_text.lower())\n",
    "        pronoun_count = len([p for p in pronouns if p.lower() != 'us' or not re.search(r'\\b(US|U\\.S\\.)\\b', text)])\n",
    "        \n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        return {\n",
    "            'POSITIVE_SCORE': positive_score,\n",
    "            'NEGATIVE_SCORE': negative_score,\n",
    "            'POLARITY_SCORE': polarity_score,\n",
    "            'SUBJECTIVITY_SCORE': subjectivity_score,\n",
    "            'AVG_SENTENCE_LENGTH': avg_sentence_length,\n",
    "            'PERCENTAGE_OF_COMPLEX_WORDS': percent_complex_words * 100,\n",
    "            'FOG_INDEX': fog_index,\n",
    "            'AVG_NUMBER_OF_WORDS_PER_SENTENCE': avg_sentence_length,\n",
    "            'COMPLEX_WORD_COUNT': len(complex_words),\n",
    "            'WORD_COUNT': len(words),\n",
    "            'SYLLABLE_PER_WORD': syllable_per_word,\n",
    "            'PERSONAL_PRONOUNS': pronoun_count,\n",
    "            'AVG_WORD_LENGTH': avg_word_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b922c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_file_write(file_path, df, max_retries=5):\n",
    "    \"\"\"Safely write DataFrame to Excel with retries\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    base_path, ext = os.path.splitext(file_path)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if attempt == 0:\n",
    "                output_path = file_path\n",
    "            else:\n",
    "                output_path = f\"{base_path}_{timestamp}{ext}\"\n",
    "            \n",
    "            df.to_excel(output_path, index=False)\n",
    "            print(f\"Successfully wrote to {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except PermissionError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed: File is locked. Retrying with different filename...\")\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                raise Exception(f\"Failed to write file after {max_retries} attempts: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        analyzer = TextAnalyzer()\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_excel('Input.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading Input.xlsx: {str(e)}\")\n",
    "            return\n",
    "        \n",
    "        results = []\n",
    "        failed_urls = []\n",
    "        \n",
    "        total_urls = len(df)\n",
    "        print(f\"Starting analysis of {total_urls} URLs...\")\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            url_id = row['URL_ID']\n",
    "            url = row['URL']\n",
    "            \n",
    "            print(f\"\\nProcessing URL {index + 1}/{total_urls}: {url_id}\")\n",
    "            \n",
    "            try:\n",
    "                title, article_text = analyzer.extract_article(url)\n",
    "                \n",
    "                if title or article_text:\n",
    "                    try:\n",
    "                        output_path = os.path.join(analyzer.article_dir, f\"{url_id}.txt\")\n",
    "                        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(f\"Title: {title}\\n\\n{article_text}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving article {url_id}: {str(e)}\")\n",
    "                \n",
    "                if article_text:\n",
    "                    analysis = analyzer.analyze_text(article_text)\n",
    "                    if analysis:\n",
    "                        analysis['URL_ID'] = url_id\n",
    "                        analysis['URL'] = url\n",
    "                        results.append(analysis)\n",
    "                else:\n",
    "                    failed_urls.append({'URL_ID': url_id, 'URL': url, 'Reason': 'No content extracted'})\n",
    "                    print(f\"No content extracted for URL {url_id}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                failed_urls.append({'URL_ID': url_id, 'URL': url, 'Reason': str(e)})\n",
    "                print(f\"Error processing URL {url_id}: {str(e)}\")\n",
    "            \n",
    "            time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        if results:\n",
    "            output_df = pd.DataFrame(results)\n",
    "            \n",
    "            columns_order = ['URL_ID', 'URL', 'POSITIVE_SCORE', 'NEGATIVE_SCORE', 'POLARITY_SCORE',\n",
    "                            'SUBJECTIVITY_SCORE', 'AVG_SENTENCE_LENGTH', 'PERCENTAGE_OF_COMPLEX_WORDS',\n",
    "                            'FOG_INDEX', 'AVG_NUMBER_OF_WORDS_PER_SENTENCE', 'COMPLEX_WORD_COUNT',\n",
    "                            'WORD_COUNT', 'SYLLABLE_PER_WORD', 'PERSONAL_PRONOUNS', 'AVG_WORD_LENGTH']\n",
    "            \n",
    "            output_df = output_df[columns_order]\n",
    "            \n",
    "            output_path = os.path.join(analyzer.output_dir, 'Output.xlsx')\n",
    "            safe_file_write(output_path, output_df)\n",
    "            \n",
    "            print(f\"\\nAnalysis complete. Successfully processed {len(results)} out of {total_urls} URLs\")\n",
    "            \n",
    "            if failed_urls:\n",
    "                failed_df = pd.DataFrame(failed_urls)\n",
    "                failed_path = os.path.join(analyzer.output_dir, 'Failed_URLs.xlsx')\n",
    "                safe_file_write(failed_path, failed_df)\n",
    "                print(f\"Failed URLs saved to Failed_URLs.xlsx ({len(failed_urls)} failures)\")\n",
    "        else:\n",
    "            print(\"\\nNo results to save. Check if articles were extracted correctly.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37f0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of 147 URLs...\n",
      "\n",
      "Processing URL 1/147: Netclan20241017\n",
      "\n",
      "Processing URL 2/147: Netclan20241018\n",
      "\n",
      "Processing URL 3/147: Netclan20241019\n",
      "\n",
      "Processing URL 4/147: Netclan20241020\n",
      "\n",
      "Processing URL 5/147: Netclan20241021\n",
      "\n",
      "Processing URL 6/147: Netclan20241022\n",
      "\n",
      "Processing URL 7/147: Netclan20241023\n",
      "\n",
      "Processing URL 8/147: Netclan20241024\n",
      "\n",
      "Processing URL 9/147: Netclan20241025\n",
      "\n",
      "Processing URL 10/147: Netclan20241026\n",
      "\n",
      "Processing URL 11/147: Netclan20241027\n",
      "\n",
      "Processing URL 12/147: Netclan20241028\n",
      "\n",
      "Processing URL 13/147: Netclan20241029\n",
      "\n",
      "Processing URL 14/147: Netclan20241030\n",
      "\n",
      "Processing URL 15/147: Netclan20241031\n",
      "\n",
      "Processing URL 16/147: Netclan20241032\n",
      "\n",
      "Processing URL 17/147: Netclan20241033\n",
      "\n",
      "Processing URL 18/147: Netclan20241034\n",
      "\n",
      "Processing URL 19/147: Netclan20241035\n",
      "\n",
      "Processing URL 20/147: Netclan20241036\n",
      "\n",
      "Processing URL 21/147: Netclan20241037\n",
      "\n",
      "Processing URL 22/147: Netclan20241038\n",
      "\n",
      "Processing URL 23/147: Netclan20241039\n",
      "\n",
      "Processing URL 24/147: Netclan20241040\n",
      "\n",
      "Processing URL 25/147: Netclan20241041\n",
      "\n",
      "Processing URL 26/147: Netclan20241042\n",
      "\n",
      "Processing URL 27/147: Netclan20241043\n",
      "\n",
      "Processing URL 28/147: Netclan20241044\n",
      "\n",
      "Processing URL 29/147: Netclan20241045\n",
      "\n",
      "Processing URL 30/147: Netclan20241046\n",
      "\n",
      "Processing URL 31/147: Netclan20241047\n",
      "\n",
      "Processing URL 32/147: Netclan20241048\n",
      "\n",
      "Processing URL 33/147: Netclan20241049\n",
      "\n",
      "Processing URL 34/147: Netclan20241050\n",
      "\n",
      "Processing URL 35/147: Netclan20241051\n",
      "\n",
      "Processing URL 36/147: Netclan20241052\n",
      "\n",
      "Processing URL 37/147: Netclan20241053\n",
      "\n",
      "Processing URL 38/147: Netclan20241054\n",
      "\n",
      "Processing URL 39/147: Netclan20241055\n",
      "\n",
      "Processing URL 40/147: Netclan20241056\n",
      "\n",
      "Processing URL 41/147: Netclan20241057\n",
      "\n",
      "Processing URL 42/147: Netclan20241058\n",
      "\n",
      "Processing URL 43/147: Netclan20241059\n",
      "\n",
      "Processing URL 44/147: Netclan20241060\n",
      "\n",
      "Processing URL 45/147: Netclan20241061\n",
      "\n",
      "Processing URL 46/147: Netclan20241062\n",
      "\n",
      "Processing URL 47/147: Netclan20241063\n",
      "\n",
      "Processing URL 48/147: Netclan20241064\n",
      "\n",
      "Processing URL 49/147: Netclan20241065\n",
      "\n",
      "Processing URL 50/147: Netclan20241066\n",
      "\n",
      "Processing URL 51/147: Netclan20241067\n",
      "\n",
      "Processing URL 52/147: Netclan20241068\n",
      "\n",
      "Processing URL 53/147: Netclan20241069\n",
      "\n",
      "Processing URL 54/147: Netclan20241070\n",
      "\n",
      "Processing URL 55/147: Netclan20241071\n",
      "\n",
      "Processing URL 56/147: Netclan20241072\n",
      "\n",
      "Processing URL 57/147: Netclan20241073\n",
      "\n",
      "Processing URL 58/147: Netclan20241074\n",
      "\n",
      "Processing URL 59/147: Netclan20241075\n",
      "\n",
      "Processing URL 60/147: Netclan20241076\n",
      "\n",
      "Processing URL 61/147: Netclan20241077\n",
      "\n",
      "Processing URL 62/147: Netclan20241078\n",
      "\n",
      "Processing URL 63/147: Netclan20241079\n",
      "\n",
      "Processing URL 64/147: Netclan20241080\n",
      "\n",
      "Processing URL 65/147: Netclan20241081\n",
      "\n",
      "Processing URL 66/147: Netclan20241082\n",
      "\n",
      "Processing URL 67/147: Netclan20241083\n",
      "\n",
      "Processing URL 68/147: Netclan20241084\n",
      "\n",
      "Processing URL 69/147: Netclan20241085\n",
      "\n",
      "Processing URL 70/147: Netclan20241086\n",
      "\n",
      "Processing URL 71/147: Netclan20241087\n",
      "\n",
      "Processing URL 72/147: Netclan20241088\n",
      "\n",
      "Processing URL 73/147: Netclan20241089\n",
      "\n",
      "Processing URL 74/147: Netclan20241090\n",
      "\n",
      "Processing URL 75/147: Netclan20241091\n",
      "\n",
      "Processing URL 76/147: Netclan20241092\n",
      "\n",
      "Processing URL 77/147: Netclan20241093\n",
      "\n",
      "Processing URL 78/147: Netclan20241094\n",
      "\n",
      "Processing URL 79/147: Netclan20241095\n",
      "\n",
      "Processing URL 80/147: Netclan20241096\n",
      "\n",
      "Processing URL 81/147: Netclan20241097\n",
      "\n",
      "Processing URL 82/147: Netclan20241098\n",
      "\n",
      "Processing URL 83/147: Netclan20241099\n",
      "\n",
      "Processing URL 84/147: Netclan20241100\n",
      "\n",
      "Processing URL 85/147: Netclan20241101\n",
      "\n",
      "Processing URL 86/147: Netclan20241102\n",
      "\n",
      "Processing URL 87/147: Netclan20241103\n",
      "\n",
      "Processing URL 88/147: Netclan20241104\n",
      "\n",
      "Processing URL 89/147: Netclan20241105\n",
      "\n",
      "Processing URL 90/147: Netclan20241106\n",
      "\n",
      "Processing URL 91/147: Netclan20241107\n",
      "\n",
      "Processing URL 92/147: Netclan20241108\n",
      "\n",
      "Processing URL 93/147: Netclan20241109\n",
      "\n",
      "Processing URL 94/147: Netclan20241110\n",
      "\n",
      "Processing URL 95/147: Netclan20241111\n",
      "\n",
      "Processing URL 96/147: Netclan20241112\n",
      "\n",
      "Processing URL 97/147: Netclan20241113\n",
      "\n",
      "Processing URL 98/147: Netclan20241114\n",
      "\n",
      "Processing URL 99/147: Netclan20241115\n",
      "\n",
      "Processing URL 100/147: Netclan20241116\n",
      "\n",
      "Processing URL 101/147: Netclan20241117\n",
      "\n",
      "Processing URL 102/147: Netclan20241118\n",
      "\n",
      "Processing URL 103/147: Netclan20241119\n",
      "\n",
      "Processing URL 104/147: Netclan20241120\n",
      "\n",
      "Processing URL 105/147: Netclan20241121\n",
      "\n",
      "Processing URL 106/147: Netclan20241122\n",
      "\n",
      "Processing URL 107/147: Netclan20241123\n",
      "\n",
      "Processing URL 108/147: Netclan20241124\n",
      "\n",
      "Processing URL 109/147: Netclan20241125\n",
      "\n",
      "Processing URL 110/147: Netclan20241126\n",
      "\n",
      "Processing URL 111/147: Netclan20241127\n",
      "\n",
      "Processing URL 112/147: Netclan20241128\n",
      "\n",
      "Processing URL 113/147: Netclan20241129\n",
      "\n",
      "Processing URL 114/147: Netclan20241130\n",
      "\n",
      "Processing URL 115/147: Netclan20241131\n",
      "\n",
      "Processing URL 116/147: Netclan20241132\n",
      "\n",
      "Processing URL 117/147: Netclan20241133\n",
      "\n",
      "Processing URL 118/147: Netclan20241134\n",
      "\n",
      "Processing URL 119/147: Netclan20241135\n",
      "\n",
      "Processing URL 120/147: Netclan20241136\n",
      "\n",
      "Processing URL 121/147: Netclan20241137\n",
      "\n",
      "Processing URL 122/147: Netclan20241138\n",
      "\n",
      "Processing URL 123/147: Netclan20241139\n",
      "\n",
      "Processing URL 124/147: Netclan20241140\n",
      "\n",
      "Processing URL 125/147: Netclan20241141\n",
      "\n",
      "Processing URL 126/147: Netclan20241142\n",
      "\n",
      "Processing URL 127/147: Netclan20241143\n",
      "\n",
      "Processing URL 128/147: Netclan20241144\n",
      "\n",
      "Processing URL 129/147: Netclan20241145\n",
      "\n",
      "Processing URL 130/147: Netclan20241146\n",
      "\n",
      "Processing URL 131/147: Netclan20241147\n",
      "\n",
      "Processing URL 132/147: Netclan20241148\n",
      "\n",
      "Processing URL 133/147: Netclan20241149\n",
      "\n",
      "Processing URL 134/147: Netclan20241150\n",
      "\n",
      "Processing URL 135/147: Netclan20241151\n",
      "\n",
      "Processing URL 136/147: Netclan20241152\n",
      "\n",
      "Processing URL 137/147: Netclan20241153\n",
      "\n",
      "Processing URL 138/147: Netclan20241154\n",
      "\n",
      "Processing URL 139/147: Netclan20241155\n",
      "\n",
      "Processing URL 140/147: Netclan20241156\n",
      "\n",
      "Processing URL 141/147: Netclan20241157\n",
      "\n",
      "Processing URL 142/147: Netclan20241158\n",
      "\n",
      "Processing URL 143/147: Netclan20241159\n",
      "\n",
      "Processing URL 144/147: Netclan20241160\n",
      "\n",
      "Processing URL 145/147: Netclan20241161\n",
      "\n",
      "Processing URL 146/147: Netclan20241162\n",
      "\n",
      "Processing URL 147/147: Netclan20241163\n",
      "Successfully wrote to output_files\\Output.xlsx\n",
      "\n",
      "Analysis complete. Successfully processed 147 out of 147 URLs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285cb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
